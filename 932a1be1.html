<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>吕小医's BLOG | 吕小医's BLOG</title><meta name="keywords" content="大数据"><meta name="author" content="lvxiaoyi"><meta name="copyright" content="lvxiaoyi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark基础-2SparkSQLSparkSQL 概述SparkSQL 是什么 Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。 Hive and SparkSQLSparkSQL 的前身是 Shark，给熟悉 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具。 Hive 是早期唯一运行在 Hadoop 上的">
<meta property="og:type" content="article">
<meta property="og:title" content="吕小医&#39;s BLOG">
<meta property="og:url" content="https://lvxiaoyi.top/932a1be1.html">
<meta property="og:site_name" content="吕小医&#39;s BLOG">
<meta property="og:description" content="Spark基础-2SparkSQLSparkSQL 概述SparkSQL 是什么 Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。 Hive and SparkSQLSparkSQL 的前身是 Shark，给熟悉 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具。 Hive 是早期唯一运行在 Hadoop 上的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.lvxiaoyi.top/typora-img/202111082049932.png/lvxiaoyi">
<meta property="article:published_time" content="2022-09-12T07:22:48.768Z">
<meta property="article:modified_time" content="2022-09-12T07:22:48.769Z">
<meta property="article:author" content="lvxiaoyi">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.lvxiaoyi.top/typora-img/202111082049932.png/lvxiaoyi"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lvxiaoyi.top/932a1be1"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '吕小医\'s BLOG',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-12 15:22:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p0.meituan.net/csc/8a1b1d488e6a8ab5108c9c78f36b3a1776955.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">205</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 小医</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img.lvxiaoyi.top/typora-img/202111082049932.png/lvxiaoyi')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">吕小医's BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 小医</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">无题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-12T07:22:48.768Z" title="发表于 2022-09-12 15:22:48">2022-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-12T07:22:48.769Z" title="更新于 2022-09-12 15:22:48">2022-09-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Spark基础-2SparkSQL"><a href="#Spark基础-2SparkSQL" class="headerlink" title="Spark基础-2SparkSQL"></a>Spark基础-2SparkSQL</h1><h1 id="SparkSQL-概述"><a href="#SparkSQL-概述" class="headerlink" title="SparkSQL 概述"></a>SparkSQL 概述</h1><h2 id="SparkSQL-是什么"><a href="#SparkSQL-是什么" class="headerlink" title="SparkSQL 是什么"></a>SparkSQL 是什么</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082049932.png/lvxiaoyi" alt="图片1"></p>
<p>Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。</p>
<h2 id="Hive-and-SparkSQL"><a href="#Hive-and-SparkSQL" class="headerlink" title="Hive and SparkSQL"></a>Hive and SparkSQL</h2><p>SparkSQL 的前身是 Shark，给熟悉 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具。</p>
<p>Hive 是早期唯一运行在 Hadoop 上的 SQL-on-Hadoop 工具。但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL-on-Hadoop的效率，大量的 SQL-on-Hadoop 工具开始产生，其中表现较为突出的是</p>
<ul>
<li><p> Drill</p>
</li>
<li><p> Impala</p>
</li>
<li><p> Shark</p>
</li>
</ul>
<p>其中 Shark 是伯克利实验室 Spark 生态环境的组件之一，是基于 Hive 所开发的工具，它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在 Spark 引擎上。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082051169.png/lvxiaoyi" alt="图片2" style="zoom:50%;" />

<p>Shark 的出现，使得 SQL-on-Hadoop 的性能比 Hive 有了 10-100 倍的提高。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082051434.png/lvxiaoyi" alt="图片3" style="zoom: 67%;" />

<p>但是，随着Spark 的发展，对于野心勃勃的Spark 团队来说，Shark 对于 Hive 的太多依赖（如采用 Hive 的语法解析器、查询优化器等等），制约了 Spark 的One Stack Rule Them All 的既定方针，制约了 Spark 各个组件的相互集成，所以提出了 SparkSQL 项目。SparkSQL 抛弃原有 Shark 的代码，汲取了 Shark 的一些优点，如内存列存储（In-Memory ColumnarStorage）、Hive 兼容性等，重新开发了SparkSQL 代码；由于摆脱了对Hive 的依赖性，SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便，真可谓“退一步，海阔天空”。</p>
<ul>
<li><p> 数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；</p>
</li>
<li><p> 性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；</p>
</li>
<li><p>组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082053385.png/lvxiaoyi"></p>
<p>2014 年 6 月 1 日 Shark 项目和 SparkSQL 项目的主持人 Reynold Xin 宣布：停止对 Shark 的开发，团队将所有资源放 SparkSQL 项目上，至此，Shark 的发展画上了句话，但也因此发展出两个支线：SparkSQL 和 Hive on Spark。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082054994.png/lvxiaoyi" alt="img"></p>
<p>其中 SparkSQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而Hive on Spark 是一个 Hive 的发展计划，该计划将 Spark 作为 Hive 的底层引擎之一，也就是说，Hive 将不再受限于一个引擎，可以采用 Map-Reduce、Tez、Spark 等引擎。</p>
<p>对于开发人员来讲，SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了 2 个编程抽象，类似 Spark Core 中的 RDD</p>
<ul>
<li><p> DataFrame</p>
</li>
<li><p> DataSet</p>
</li>
</ul>
<h2 id="SparkSQL-特点"><a href="#SparkSQL-特点" class="headerlink" title="SparkSQL 特点"></a>SparkSQL 特点</h2><ul>
<li><p>易整合</p>
<ul>
<li>无缝的整合了 SQL 查询和 Spark 编程</li>
</ul>
</li>
<li><p>统一的数据访问</p>
</li>
<li><p>使用相同的方式连接不同的数据源</p>
</li>
<li><p>兼容 Hive</p>
</li>
<li><p>标准数据连接</p>
</li>
</ul>
<h2 id="DataFrame-是什么"><a href="#DataFrame-是什么" class="headerlink" title="DataFrame 是什么"></a>DataFrame 是什么</h2><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。</p>
<p>同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082057436.png/lvxiaoyi" alt="image-20211108205750369"></p>
<p>上图直观地体现了 DataFrame 和 RDD 的区别。</p>
<p>左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</p>
<p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待</p>
<p>DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。比如下面一个例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users.join(events,users(&quot;id&quot; ) === events(&quot;uid&quot;)).filter(events( &quot;date&quot;) &gt;&quot;2015-01-01&quot;)</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202112031110004.png/lvxiaoyi" alt="image-20211203111003705"></p>
<p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们 join 之后又做了一次 filter 操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 join 是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将 filter 下推到 join 下方，先对 DataFrame 进行过滤，再 join 过滤后的较小的结果集，便可以有效缩短执行时间。而 Spark SQL 的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082059993.png/lvxiaoyi" alt="image-20211108205945923"></p>
<h2 id="DataSet-是什么"><a href="#DataSet-是什么" class="headerlink" title="DataSet 是什么"></a>DataSet 是什么</h2><p>DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。</p>
<ul>
<li><p>DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象</p>
</li>
<li><p>用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性；</p>
</li>
<li><p>用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</p>
</li>
<li><p>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。 </p>
</li>
<li><p>DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</p>
</li>
</ul>
<h1 id="SparkSQL-核心编程"><a href="#SparkSQL-核心编程" class="headerlink" title="SparkSQL 核心编程"></a>SparkSQL 核心编程</h1><p>本课件重点学习如何使用 Spark SQL 所提供的 DataFrame 和 DataSet 模型进行编程.，以及了解它们之间的关系和转换，关于具体的 SQL 书写不是我们的重点。</p>
<h2 id="新的起点"><a href="#新的起点" class="headerlink" title="新的起点"></a>新的起点</h2><p>Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext，Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。</p>
<p>在老的版本中，SparkSQL 提供两种 SQL 查询起始点：一个叫 SQLContext，用于 Spark自己提供的 SQL 查询；一个叫 HiveContext，用于连接 Hive 的查询。</p>
<p>SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext的组合，所以在 SQLContex 和 HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。SparkSession 内部封装了 SparkContext，所以计算实际上是由 sparkContext 完成的。当我们使用 spark-shell 的时候, spark 框架会自动的创建一个名称叫做 spark 的 SparkSession 对 象, 就像我们以前可以自动获取到一个 sc 来表示 SparkContext 对象一样</p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>Spark SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。DataFrame API 既有 transformation 操作也有 action 操作。 </p>
<h3 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h3><p>在 Spark SQL 中 SparkSession 是创建 DataFrame 和执行 SQL 的入口，创建 DataFrame有三种方式：通过 Spark 的数据源进行创建；从一个存在的 RDD 进行转换；还可以从 Hive Table 进行查询返回。</p>
<ol>
<li><p>从 Spark 数据源进行创建</p>
<ul>
<li><p>查看 Spark 支持创建文件的数据源格式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv format jdbc json load option options orc parquet schema </span><br><span class="line">table text textFile</span><br></pre></td></tr></table></figure></li>
<li><p>在 spark 的 bin/data 目录中创建 user.json 文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;username&quot;:&quot;zhangsan&quot;,&quot;age&quot;:20&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>读取 json 文件创建 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
</ul>
<p>注意：如果从内存中获取数据，spark 可以知道数据类型具体是什么。如果是数字，默认作为 Int 处理；但是从文件中读取的数字，不能确定是什么类型，所以用 bigint 接收，可以和Long 类型转换，但是和 Int 不能进行转换</p>
<p>展示结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 20|zhangsan|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure></li>
<li><p>从 RDD 进行转换</p>
<p>在后续章节中讨论</p>
</li>
<li><p>从 Hive Table 进行查询返回</p>
<p>在后续章节中讨论</p>
</li>
</ol>
<h3 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h3><p>SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助</p>
<ol>
<li><p>读取 JSON 文件创建 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
<li><p>对 DataFrame 创建一个临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>通过 SQL 语句实现查询全表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)</span><br><span class="line">sqlDF: org.apache.spark.sql.DataFrame = [age: bigint， name: string]</span><br></pre></td></tr></table></figure></li>
<li><p>结果展示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sqlDF.show</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 20|zhangsan|</span><br><span class="line">| 30| lisi|</span><br><span class="line">| 40| wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<p>注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使用全局临时表时需要全路径访问，如：global_temp.people</p>
</li>
<li><p>对于 DataFrame 创建一个全局表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createGlobalTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>通过 SQL 语句实现查询全表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 20|zhangsan|</span><br><span class="line">| 30| lisi|</span><br><span class="line">| 40| wangwu|</span><br><span class="line">+---+--------+</span><br><span class="line">scala&gt; spark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 20|zhangsan|</span><br><span class="line">| 30| lisi|</span><br><span class="line">| 40| wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="DSL-语法"><a href="#DSL-语法" class="headerlink" title="DSL 语法"></a>DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了</p>
<ol>
<li><p>创建一个 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint， name: string]</span><br></pre></td></tr></table></figure></li>
<li><p>查看 DataFrame 的 Schema 信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line">|-- age: Long (nullable = true)</span><br><span class="line">|-- username: string (nullable = true)</span><br></pre></td></tr></table></figure></li>
<li><p>只查看”username”列数据，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(&quot;username&quot;).show()</span><br><span class="line">+--------+</span><br><span class="line">|username|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">| lisi|</span><br><span class="line">| wangwu|</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure></li>
<li><p>查看”username”列数据以及”age+1”数据</p>
<p>注意:涉及到运算的时候, 每列都必须使用$, 或者采用引 号表达式：单引号+字段名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($&quot;username&quot;,$&quot;age&quot; + 1).show</span><br><span class="line">scala&gt; df.select(&#x27;username, &#x27;age + 1).show()</span><br><span class="line">scala&gt; df.select(&#x27;username, &#x27;age + 1 as &quot;newage&quot;).show()</span><br><span class="line">+--------+---------+</span><br><span class="line">|username|(age + 1)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan| 21|</span><br><span class="line">| lisi| 31|</span><br><span class="line">| wangwu| 41|</span><br><span class="line">+--------+---------+</span><br></pre></td></tr></table></figure></li>
<li><p>查看”age”大于”30”的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($&quot;age&quot;&gt;30).show</span><br><span class="line">+---+---------+</span><br><span class="line">|age| username|</span><br><span class="line">+---+---------+</span><br><span class="line">| 40| wangwu|</span><br><span class="line">+---+---------+</span><br></pre></td></tr></table></figure></li>
<li><p>按照”age”分组，查看数据条数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(&quot;age&quot;).count.show</span><br><span class="line">+---+-----+</span><br><span class="line">|age|count|</span><br><span class="line">+---+-----+</span><br><span class="line">| 20| 1|</span><br><span class="line">| 30| 1|</span><br><span class="line">| 40| 1|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="RDD-转换为-DataFrame"><a href="#RDD-转换为-DataFrame" class="headerlink" title="RDD 转换为 DataFrame"></a>RDD 转换为 DataFrame</h3><p>在 IDEA 中开发程序时，如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入import spark.implicits._</p>
<p>这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 Scala 只支持val 修饰的对象的引入。</p>
<p>spark-shell 中无需导入，自动完成此操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val idRDD = sc.textFile(&quot;data/id.txt&quot;)</span><br><span class="line">scala&gt; idRDD.toDF(&quot;id&quot;).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">| 1|</span><br><span class="line">| 2|</span><br><span class="line">| 3|</span><br><span class="line">| 4|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>实际开发中，一般通过样例类将 RDD 转换为 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class User(name:String, age:Int)</span><br><span class="line">defined class User</span><br><span class="line">scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, </span><br><span class="line">t._2)).toDF.show</span><br><span class="line">+--------+---+</span><br><span class="line">| name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| 30|</span><br><span class="line">| lisi| 40|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure>

<h3 id="DataFrame-转换为-RDD"><a href="#DataFrame-转换为-RDD" class="headerlink" title="DataFrame 转换为 RDD"></a>DataFrame 转换为 RDD</h3><p>DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, </span><br><span class="line">t._2)).toDF</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br><span class="line">scala&gt; val rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] </span><br><span class="line">at rdd at &lt;console&gt;:25</span><br><span class="line">scala&gt; val array = rdd.collect</span><br><span class="line">array: Array[org.apache.spark.sql.Row] = Array([zhangsan,30], [lisi,40])</span><br></pre></td></tr></table></figure>

<p>注意：此时得到的 RDD 存储类型为 Row</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; array(0)</span><br><span class="line">res28: org.apache.spark.sql.Row = [zhangsan,30]</span><br><span class="line">scala&gt; array(0)(0)</span><br><span class="line">res29: Any = zhangsan</span><br><span class="line">scala&gt; array(0).getAs[String](&quot;name&quot;)</span><br><span class="line">res30: String = zhangsan</span><br></pre></td></tr></table></figure>

<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。</p>
<h3 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h3><ol>
<li><p>使用样例类序列创建 DataSet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Person(name: String, age: Long)</span><br><span class="line">defined class Person</span><br><span class="line">scala&gt; val caseClassDS = Seq(Person(&quot;zhangsan&quot;,2)).toDS()</span><br><span class="line">caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]</span><br><span class="line">scala&gt; caseClassDS.show</span><br><span class="line">+---------+---+</span><br><span class="line">| name    |age|</span><br><span class="line">+---------+---+</span><br><span class="line">| zhangsan| 2|</span><br><span class="line">+---------+---+</span><br></pre></td></tr></table></figure></li>
<li><p>使用基本类型的序列创建 DataSet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val ds = Seq(1,2,3,4,5).toDS</span><br><span class="line">ds: org.apache.spark.sql.Dataset[Int] = [value: int]</span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| 1|</span><br><span class="line">| 2|</span><br><span class="line">| 3|</span><br><span class="line">| 4|</span><br><span class="line">| 5|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>



<p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</p>
</li>
</ol>
<h3 id="RDD-转换为-DataSet"><a href="#RDD-转换为-DataSet" class="headerlink" title="RDD 转换为 DataSet"></a>RDD 转换为 DataSet</h3><p>SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class User(name:String, age:Int)</span><br><span class="line">defined class User</span><br><span class="line">scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, </span><br><span class="line">t._2)).toDS</span><br><span class="line">res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h3 id="DataSet-转换为-RDD"><a href="#DataSet-转换为-RDD" class="headerlink" title="DataSet 转换为 RDD"></a>DataSet 转换为 RDD</h3><p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class User(name:String, age:Int)</span><br><span class="line">defined class User</span><br><span class="line">scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, </span><br><span class="line">t._2)).toDS</span><br><span class="line">res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br><span class="line">scala&gt; val rdd = res11.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at </span><br><span class="line">&lt;console&gt;:25</span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res12: Array[User] = Array(User(zhangsan,30), User(lisi,49))</span><br></pre></td></tr></table></figure>

<h2 id="DataFrame-和-DataSet-转换"><a href="#DataFrame-和-DataSet-转换" class="headerlink" title="DataFrame 和 DataSet 转换"></a>DataFrame 和 DataSet 转换</h2><p>DataFrame 其实是 DataSet 的 特例，所以它们之间是可以互相转换的。</p>
<ol>
<li><p>DataFrame 转换为 DataSet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class User(name:String, age:Int)</span><br><span class="line">defined class User</span><br><span class="line">scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), </span><br><span class="line">(&quot;lisi&quot;,49))).toDF(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br><span class="line">scala&gt; val ds = df.as[User]</span><br><span class="line">ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br></pre></td></tr></table></figure></li>
<li><p>DataSet 转换为 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val ds = df.as[User]</span><br><span class="line">ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br><span class="line">scala&gt; val df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="RDD、DataFrame、DataSet-三者的关系"><a href="#RDD、DataFrame、DataSet-三者的关系" class="headerlink" title="RDD、DataFrame、DataSet 三者的关系"></a>RDD、DataFrame、DataSet 三者的关系</h2><p>在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看：</p>
<ul>
<li><p>Spark1.0 =&gt; RDD </p>
</li>
<li><p>Spark1.3 =&gt; DataFrame</p>
</li>
<li><p> Spark1.6 =&gt; Dataset</p>
</li>
</ul>
<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的 Spark 版本中，DataSet 有可能会逐步取代 RDD和 DataFrame 成为唯一的 API 接口。</p>
<h3 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h3><ul>
<li><p>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利; </p>
</li>
<li><p>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算; </p>
</li>
<li><p>三者有许多共同的函数，如 filter，排序等; </p>
</li>
<li><p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）</p>
</li>
<li><p>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p>
</li>
<li><p>三者都有 partition 的概念</p>
</li>
<li><p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p>
</li>
</ul>
<h3 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h3><ol>
<li><p>RDD</p>
<ul>
<li><p>RDD 一般和 spark mllib 同时使用</p>
</li>
<li><p>RDD 不支持 sparksql 操作</p>
</li>
</ul>
</li>
<li><p>DataFrame</p>
<ul>
<li><p>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</p>
</li>
<li><p>DataFrame 与 DataSet 一般不与 spark mllib 同时使用</p>
</li>
<li><p>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作</p>
</li>
<li><p>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</p>
</li>
</ul>
</li>
<li><p>DataSet</p>
<ul>
<li><p>Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。</p>
<p>DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]</p>
</li>
<li><p>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</p>
</li>
</ul>
</li>
</ol>
<h3 id="三者的互相转换"><a href="#三者的互相转换" class="headerlink" title="三者的互相转换"></a>三者的互相转换</h3><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202112031418124.png/lvxiaoyi" alt="image-20211203141836999" style="zoom:80%;" />

<h2 id="IDEA-开发-SparkSQL"><a href="#IDEA-开发-SparkSQL" class="headerlink" title="IDEA 开发 SparkSQL"></a>IDEA 开发 SparkSQL</h2><p>实际开发中，都是使用 IDEA 进行开发的。</p>
<h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL01_Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文环境配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL01_Demo&quot;</span>)</span><br><span class="line">    <span class="comment">//创建 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">    <span class="comment">//RDD=&gt;DataFrame=&gt;DataSet 转换需要引入隐式转换规则，否则无法转换</span></span><br><span class="line">    <span class="comment">//spark 不是包名，是上下文环境对象名</span></span><br><span class="line">    <span class="comment">//读取 json 文件 创建 DataFrame &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;input/test.json&quot;</span>)</span><br><span class="line">    <span class="comment">//df.show()</span></span><br><span class="line">    <span class="comment">//SQL 风格语法</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">    <span class="comment">//spark.sql(&quot;select avg(age) from user&quot;).show</span></span><br><span class="line">    <span class="comment">//DSL 风格语法</span></span><br><span class="line">    <span class="comment">//df.select(&quot;username&quot;,&quot;age&quot;).show()</span></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataFrame=&gt;DataSet*****</span></span><br><span class="line">    <span class="comment">//RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] =</span><br><span class="line">    spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="string">&quot;lisi&quot;</span>, <span class="number">28</span>), (<span class="number">3</span>, <span class="string">&quot;wangwu&quot;</span>,</span><br><span class="line">      <span class="number">20</span>)))</span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df1: <span class="type">DataFrame</span> = rdd1.toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">    <span class="comment">//df1.show()</span></span><br><span class="line">    <span class="comment">//DateSet</span></span><br><span class="line">    <span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">User</span>] = df1.as[<span class="type">User</span>]</span><br><span class="line">    <span class="comment">//ds1.show()</span></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;DataFrame=&gt;RDD*****</span></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = ds1.toDF()</span><br><span class="line">    <span class="comment">//RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集，但是索引从0 开始</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Row</span>] = df2.rdd</span><br><span class="line">    <span class="comment">//rdd2.foreach(a=&gt;println(a.getString(1)))</span></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataSet*****</span></span><br><span class="line">    rdd1.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (id, name, age) =&gt; <span class="type">User</span>(id, name, age)</span><br><span class="line">    &#125;.toDS()</span><br><span class="line">    <span class="comment">//*****DataSet=&gt;=&gt;RDD*****</span></span><br><span class="line">    ds1.rdd</span><br><span class="line">    <span class="comment">//释放资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能。</p>
<h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><ol>
<li><p>创建 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
<li><p>注册 UDF</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.udf.register(&quot;addName&quot;,(x:String)=&gt; &quot;Name:&quot;+x)</span><br><span class="line">res9: org.apache.spark.sql.expressions.UserDefinedFunction = </span><br><span class="line">UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))</span><br></pre></td></tr></table></figure></li>
<li><p>创建临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>应用 UDF</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;Select addName(name),age from people&quot;).show()</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><p>强类型的 Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数Aggregator</p>
<p><strong>需求：计算平均工资</strong></p>
<p>一个需求可以采用很多种不同的方法实现需求</p>
<ol>
<li><p>实现方式 - RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val conf: SparkConf = new SparkConf().setAppName(&quot;app&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">val sc: SparkContext = new SparkContext(conf)</span><br><span class="line">val res: (Int, Int) = sc.makeRDD(List((&quot;zhangsan&quot;, 20), (&quot;lisi&quot;, 30), (&quot;wangw&quot;, 40))).map &#123;</span><br><span class="line"> case (name, age) =&gt; &#123;</span><br><span class="line"> (age, 1)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;.reduce &#123;</span><br><span class="line"> (t1, t2) =&gt; &#123;</span><br><span class="line">  (t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line"> &#125; &#125;</span><br><span class="line">println(res._1/res._2)</span><br><span class="line">// 关闭连接</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - 累加器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAC</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sum: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> count: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">return</span> sum == <span class="number">0</span> &amp;&amp; count == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newMyAc = <span class="keyword">new</span> <span class="type">MyAC</span></span><br><span class="line">    newMyAc.sum = <span class="keyword">this</span>.sum</span><br><span class="line">    newMyAc.count = <span class="keyword">this</span>.count</span><br><span class="line">    newMyAc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum += v</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o: <span class="type">MyAC</span> =&gt; &#123;</span><br><span class="line">        sum += o.sum</span><br><span class="line">        count += o.count</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Int</span> = sum / count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - UDAF - 弱类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">定义类继承 UserDefinedAggregateFunction，并重写其中方法</span></span><br><span class="line"><span class="comment">UserDefinedAggregateFunction已经被遗弃</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> =</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">IntegerType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数缓冲区中值的数据类型(age,count)</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>), <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数返回值的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 稳定性：对于相同的输入是否一直返回相同的输出。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数缓冲区初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 存年龄的总和</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">// 存年龄的个数</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更新缓冲区中的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getInt(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并缓冲区</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble /</span><br><span class="line">    buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="comment">//创建聚合函数</span></span><br><span class="line"><span class="keyword">var</span> myAverage = <span class="keyword">new</span> <span class="type">MyAveragUDAF</span></span><br><span class="line"><span class="comment">//在 spark 中注册聚合函数</span></span><br><span class="line">spark.udf.register (<span class="string">&quot;avgAge&quot;</span>, myAverage)</span><br><span class="line">spark.sql (<span class="string">&quot;select avgAge(age) from user&quot;</span>).show ()</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - UDAF - 强类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//输入数据类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User01</span>(<span class="params">username: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//缓存类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AgeBuffer</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 定义类继承 org.apache.spark.sql.expressions.Aggregator</span></span><br><span class="line"><span class="comment"> * 重写类中的方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF1</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User01</span>, <span class="type">AgeBuffer</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    <span class="type">AgeBuffer</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">AgeBuffer</span>, a: <span class="type">User01</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b.sum = b.sum + a.age</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">AgeBuffer</span>, b2: <span class="type">AgeBuffer</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">AgeBuffer</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    buff.sum.toDouble / buff.count</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//DataSet 默认额编解码器，用于序列化，固定写法</span></span><br><span class="line">  <span class="comment">//自定义类型就是 product 自带类型根据类型选择</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">AgeBuffer</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.product</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"><span class="comment">//封装为 DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User01</span>] = df.as[<span class="type">User01</span>]</span><br><span class="line"><span class="comment">//创建聚合函数</span></span><br><span class="line"><span class="keyword">var</span> myAgeUdaf1 = <span class="keyword">new</span> <span class="type">MyAveragUDAF1</span></span><br><span class="line"><span class="comment">//将聚合函数转换为查询的列</span></span><br><span class="line"><span class="keyword">val</span> col: <span class="type">TypedColumn</span>[<span class="type">User01</span>, <span class="type">Double</span>] = myAgeUdaf1.toColumn</span><br><span class="line"><span class="comment">//查询</span></span><br><span class="line">ds.select (col).show ()</span><br></pre></td></tr></table></figure>

<p>Spark3.0 版本可以采用强类型的 Aggregator 方式代替 UserDefinedAggregateFunction </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO 创建 UDAF 函数</span></span><br><span class="line"><span class="keyword">val</span> udaf = <span class="keyword">new</span> <span class="type">MyAvgAgeUDAF</span></span><br><span class="line"><span class="comment">// TODO 注册到 SparkSQL 中</span></span><br><span class="line">spark.udf.register (<span class="string">&quot;avgAge&quot;</span>, functions.udaf (udaf) )</span><br><span class="line"><span class="comment">// TODO 在 SQL 中使用聚合函数</span></span><br><span class="line"><span class="comment">// 定义用户的自定义聚合函数</span></span><br><span class="line">spark.sql (<span class="string">&quot;select avgAge(age) from user&quot;</span>).show</span><br><span class="line"></span><br><span class="line"><span class="comment">// **************************************************</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params">var sum: <span class="type">Long</span>, var cnt: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// totalage, count</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAvgAgeUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = <span class="type">Buff</span>(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Buff</span>, a: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">    b.sum += a</span><br><span class="line">    b.cnt += <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Buff</span>, b2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.cnt += b2.cnt</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Buff</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    reduction.sum.toDouble / reduction.cnt</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h2><h3 id="通用的加载和保存方式"><a href="#通用的加载和保存方式" class="headerlink" title="通用的加载和保存方式"></a>通用的加载和保存方式</h3><p>SparkSQL 提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL 默认读取和保存的文件格式为 parquet</p>
<ol>
<li><p>加载数据</p>
<p>spark.read.load 是加载数据的通用方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv format jdbc json load option options orc parquet schema </span><br><span class="line">table text textFile</span><br></pre></td></tr></table></figure>

<p>如果读取不同格式的数据，可以对不同的数据格式进行设定</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>format(“…”)：指定加载的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”。 </p>
</li>
<li><p>load(“…”)：在”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”格式下需要传入加载数据的路径。</p>
</li>
<li><p>option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable</p>
</li>
</ul>
<p>我们前面都是使用 read API 先把文件加载到 DataFrame 然后再查询，其实，我们也可以直</p>
<p>接在文件上进行查询: 文件格式.`文件路径`</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;spark.sql(&quot;select * from json.`/opt/module/data/user.json`&quot;).show</span><br></pre></td></tr></table></figure></li>
<li><p>保存数据</p>
<p>df.write.save 是保存数据的通用方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.</span><br><span class="line">csv jdbc json orc parquet textFile… …</span><br></pre></td></tr></table></figure>

<p>如果保存不同格式的数据，可以对不同的数据格式进行设定</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>format(“…”)：指定保存的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”。 </p>
</li>
<li><p> save (“…”)：在”csv”、”orc”、”parquet”和”textFile”格式下需要传入保存数据的路径。</p>
</li>
<li><p> option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable</p>
</li>
</ul>
<p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。</p>
<p>有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。</p>
<p>SaveMode 是一个枚举类，其中的常量包括：</p>
<table>
<thead>
<tr>
<th>Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody></table>
<p>df.write.mode(“append”).json(“/opt/module/data/output”)</p>
</li>
</ol>
<h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。</p>
<p>数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。修改配置项 spark.sql.sources.default，可修改默认数据源格式。</p>
<ol>
<li><p>加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.load(&quot;examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">scala&gt; df.show</span><br></pre></td></tr></table></figure></li>
<li><p>保存数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var df = spark.read.json(&quot;/opt/module/data/input/people.json&quot;)</span><br><span class="line">//保存为 parquet 格式</span><br><span class="line">scala&gt; df.write.mode(&quot;append&quot;).save(&quot;/opt/module/data/output&quot;)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。</p>
<p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串。格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125;</span><br><span class="line">[&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;]</span><br></pre></td></tr></table></figure>

<ol>
<li><p>导入隐式转换</p>
<p>import spark.implicits._</p>
</li>
<li><p>加载 JSON 文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val path = &quot;/opt/module/spark-local/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br></pre></td></tr></table></figure></li>
<li><p>创建临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>数据查询</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 </span><br><span class="line">AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">+------+</span><br><span class="line">| name|</span><br><span class="line">+------+</span><br><span class="line">|Justin|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为数据列</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, </span><br><span class="line">&quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data/user.csv&quot;)</span><br></pre></td></tr></table></figure>



<h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell </span><br><span class="line">--jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>

<p>我们这里只演示在 Idea 中通过 JDBC 对 Mysql 进行操作</p>
<ol>
<li><p>导入依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line"> &lt;version&gt;5.1.27&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>读取数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="comment">//创建 SparkSession 对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"><span class="comment">//方式 1：通用的 load 方法读取</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;user&quot;</span>)</span><br><span class="line">.load().show</span><br><span class="line"><span class="comment">//方式 2:通用的 load 方法读取 参数另一种形式</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.options(<span class="type">Map</span>(<span class="string">&quot;url&quot;</span> -&gt; <span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;</span>,</span><br><span class="line">             <span class="string">&quot;dbtable&quot;</span> -&gt; <span class="string">&quot;user&quot;</span>, <span class="string">&quot;driver&quot;</span> -&gt; <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)).load().show</span><br><span class="line"><span class="comment">//方式 3:使用 jdbc 方法读取</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>, <span class="string">&quot;user&quot;</span>, props)</span><br><span class="line">df.show</span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></li>
<li><p>写入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">case class User2(name: String, age: Long)</span><br><span class="line">。。。</span><br><span class="line">val conf: SparkConf = new </span><br><span class="line">SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)</span><br><span class="line">//创建 SparkSession 对象</span><br><span class="line">val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()</span><br><span class="line">import spark.implicits._</span><br><span class="line">val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2(&quot;lisi&quot;, 20), User2(&quot;zs&quot;, 30)))</span><br><span class="line">val ds: Dataset[User2] = rdd.toDS</span><br><span class="line">//方式 1：通用的方式 format 指定写出类型</span><br><span class="line">ds.write</span><br><span class="line"> .format(&quot;jdbc&quot;)</span><br><span class="line"> .option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)</span><br><span class="line"> .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line"> .option(&quot;password&quot;, &quot;123123&quot;)</span><br><span class="line"> .option(&quot;dbtable&quot;, &quot;user&quot;)</span><br><span class="line"> .mode(SaveMode.Append)</span><br><span class="line"> .save()</span><br><span class="line">//方式 2：通过 jdbc 方法</span><br><span class="line">val props: Properties = new Properties()</span><br><span class="line">props.setProperty(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">props.setProperty(&quot;password&quot;, &quot;123123&quot;)</span><br><span class="line">ds.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, </span><br><span class="line">&quot;user&quot;, props)</span><br><span class="line">//释放资源</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)</p>
<p>以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译 Spark SQL 时引入 Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p>
<p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到Spark 的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p>
<p>spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。</p>
<ol>
<li><p>内嵌的 HIVE</p>
<p>如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.<br>Hive 的元数据存储在 derby 中, 默认仓库地址:$SPARK_HOME/spark-warehouse</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;show tables&quot;).show</span><br><span class="line">。。。</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+ +--------+---------+-----------+</span><br><span class="line">scala&gt; spark.sql(&quot;create table aa(id int)&quot;)</span><br><span class="line">。。。</span><br><span class="line">scala&gt; spark.sql(&quot;show tables&quot;).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| default| aa| false|</span><br><span class="line">+--------+---------+-----------+</span><br></pre></td></tr></table></figure>

<p>向表加载本地数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;load data local inpath &#x27;input/ids.txt&#x27; into table aa&quot;)</span><br><span class="line">。。。</span><br><span class="line">scala&gt; spark.sql(&quot;select * from aa&quot;).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">| 1|</span><br><span class="line">| 2|</span><br><span class="line">| 3|</span><br><span class="line">| 4|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>在实际使用中, 几乎没有任何人会使用内置的 Hive</p>
</li>
<li><p>外部的 HIVE</p>
<p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：</p>
<ul>
<li><p>Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下</p>
</li>
<li><p>把 Mysql 的驱动 copy 到 jars/目录下</p>
</li>
<li><p>如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下</p>
</li>
<li><p>重启 spark-shell </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;show tables&quot;).show</span><br><span class="line">20/04/25 22:05:14 WARN ObjectStore: Failed to get database global_temp, returning </span><br><span class="line">NoSuchObjectException</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">|database| tableName|isTemporary|</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">| default| emp| false|</span><br><span class="line">| default|hive_hbase_emp_table| false|</span><br><span class="line">| default| relevance_hbase_emp| false|</span><br><span class="line">| default| staff_hive| false|</span><br><span class="line">| default| ttt| false|</span><br><span class="line">| default| user_visit_action| false|</span><br><span class="line">+--------+--------------------+-----------+</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>运行 Spark SQL CLI.</p>
<p>Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在<br>Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口</p>
<p>bin/spark-sql</p>
</li>
<li><p>运行 Spark beeline</p>
<p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。如果想连接 Thrift Server，需要通过以下几个步骤：</p>
<ul>
<li><p>Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下</p>
</li>
<li><p>把 Mysql 的驱动 copy 到 jars/目录下</p>
</li>
<li><p>如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下</p>
</li>
<li><p>启动 Thrift Server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure></li>
<li><p>使用 beeline 连接 Thrift Server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/beeline -u jdbc:hive2://linux1:10000 -n root</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>代码操作 Hive</p>
<ol>
<li><p>导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>将 hive-site.xml 文件拷贝到项目的 resources 目录中，代码实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//创建 SparkSession</span><br><span class="line">val spark: SparkSession = SparkSession</span><br><span class="line"> .builder()</span><br><span class="line"> .enableHiveSupport()</span><br><span class="line"> .master(&quot;local[*]&quot;)</span><br><span class="line"> .appName(&quot;sql&quot;)</span><br><span class="line"> .getOrCreate()</span><br></pre></td></tr></table></figure></li>
</ol>
<p>注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: </p>
<p>config(“spark.sql.warehouse.dir”, “hdfs://linux1:8020/user/hive/warehouse”)</p>
<p>如果在执行操作时，出现如下错误：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Permission denied: user=18801，access=WRITE, inode=&quot;/user/hive/warehouse/userid&quot;</span><br></pre></td></tr></table></figure>

<p>可以代码最前面增加如下代码解决：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)</span><br></pre></td></tr></table></figure>

<p>此处的 root 改为你们自己的 hadoop 用户名称</p>
</li>
</ol>
<h1 id="SparkSQL-项目实战"><a href="#SparkSQL-项目实战" class="headerlink" title="SparkSQL 项目实战"></a>SparkSQL 项目实战</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>我们这次 Spark-sql 操作中所有的数据均来自 Hive，首先在 Hive 中创建表,，并导入数据。</p>
<p>一共有 3 张表： 1 张用户行为表，1 张城市表，1 张产品表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `user_visit_action`(</span><br><span class="line">`<span class="type">date</span>` string,</span><br><span class="line">`user_id` <span class="type">bigint</span>,</span><br><span class="line">`session_id` string,</span><br><span class="line">`page_id` <span class="type">bigint</span>,</span><br><span class="line">`action_time` string,</span><br><span class="line">`search_keyword` string,</span><br><span class="line">`click_category_id` <span class="type">bigint</span>,</span><br><span class="line">`click_product_id` <span class="type">bigint</span>,</span><br><span class="line">`order_category_ids` string,</span><br><span class="line">`order_product_ids` string,</span><br><span class="line">`pay_category_ids` string,</span><br><span class="line">`pay_product_ids` string,</span><br><span class="line">`city_id` <span class="type">bigint</span>)</span><br><span class="line"></span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/input/user_visit_action.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> user_visit_action;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `product_info`(</span><br><span class="line">`product_id` <span class="type">bigint</span>,</span><br><span class="line">`product_name` string,</span><br><span class="line">`extend_info` string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/input/product_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> product_info;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `city_info`(</span><br><span class="line">`city_id` <span class="type">bigint</span>,</span><br><span class="line">`city_name` string,</span><br><span class="line">`area` string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/input/city_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> city_info;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.spark.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark06_SparkSQL_Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use atguigu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 准备数据</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `user_visit_action`(</span></span><br><span class="line"><span class="string">              |  `date` string,</span></span><br><span class="line"><span class="string">              |  `user_id` bigint,</span></span><br><span class="line"><span class="string">              |  `session_id` string,</span></span><br><span class="line"><span class="string">              |  `page_id` bigint,</span></span><br><span class="line"><span class="string">              |  `action_time` string,</span></span><br><span class="line"><span class="string">              |  `search_keyword` string,</span></span><br><span class="line"><span class="string">              |  `click_category_id` bigint,</span></span><br><span class="line"><span class="string">              |  `click_product_id` bigint,</span></span><br><span class="line"><span class="string">              |  `order_category_ids` string,</span></span><br><span class="line"><span class="string">              |  `order_product_ids` string,</span></span><br><span class="line"><span class="string">              |  `pay_category_ids` string,</span></span><br><span class="line"><span class="string">              |  `pay_product_ids` string,</span></span><br><span class="line"><span class="string">              |  `city_id` bigint)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |load data local inpath &#x27;datas/user_visit_action.txt&#x27; into table atguigu.user_visit_action</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `product_info`(</span></span><br><span class="line"><span class="string">              |  `product_id` bigint,</span></span><br><span class="line"><span class="string">              |  `product_name` string,</span></span><br><span class="line"><span class="string">              |  `extend_info` string)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |load data local inpath &#x27;datas/product_info.txt&#x27; into table atguigu.product_info</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `city_info`(</span></span><br><span class="line"><span class="string">              |  `city_id` bigint,</span></span><br><span class="line"><span class="string">              |  `city_name` string,</span></span><br><span class="line"><span class="string">              |  `area` string)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |load data local inpath &#x27;datas/city_info.txt&#x27; into table atguigu.city_info</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;&quot;&quot;select * from city_info&quot;&quot;&quot;</span>).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="需求：各区域热门商品-Top3"><a href="#需求：各区域热门商品-Top3" class="headerlink" title="需求：各区域热门商品 Top3"></a>需求：各区域热门商品 Top3</h2><h3 id="需求简介"><a href="#需求简介" class="headerlink" title="需求简介"></a>需求简介</h3><p>这里的热门商品是从点击量的维度来看的，计算各个区域前三大热门商品，并备注上每个商品在主要城市中的分布比例，超过两个城市用其他显示。</p>
<p>例如：</p>
<table>
<thead>
<tr>
<th><strong>地区</strong></th>
<th><strong>商品名称</strong></th>
<th><strong>点击次数</strong></th>
<th><strong>城市备注</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>华北</strong></td>
<td>商品 A</td>
<td>100000</td>
<td>北京 21.2%，天津 13.2%，其他 65.6%</td>
</tr>
<tr>
<td><strong>华北</strong></td>
<td>商品 P</td>
<td>80200</td>
<td>北京 63.0%，太原 10%，其他 27.0%</td>
</tr>
<tr>
<td><strong>华北</strong></td>
<td>商品 M</td>
<td>40000</td>
<td>北京 63.0%，太原 10%，其他 27.0%</td>
</tr>
<tr>
<td><strong>东北</strong></td>
<td>商品 J</td>
<td>92000</td>
<td>大连 28%，辽宁 17.0%，其他 55.0%</td>
</tr>
</tbody></table>
<h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><ul>
<li><p>查询出来所有的点击记录，并与 city_info 表连接，得到每个城市所在的地区，与</p>
<p>Product_info 表连接得到产品名称</p>
</li>
<li><p>按照地区和商品 id 分组，统计出每个商品在每个地区的总点击次数</p>
</li>
<li><p>每个地区内按照点击次数降序排列</p>
</li>
<li><p>只取前三名</p>
</li>
<li><p>城市备注需要自定义 UDAF 函数</p>
</li>
</ul>
<h3 id="功能实现"><a href="#功能实现" class="headerlink" title="功能实现"></a>功能实现</h3><ul>
<li>连接三张表的数据，获取完整的数据（只有点击）</li>
<li>将数据根据地区，商品名称分组</li>
<li>统计商品点击次数总和,取 Top3</li>
<li>实现自定义聚合函数显示备注 </li>
</ul>
<p>第一步：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"> package com.atguigu.bigdata.spark.sql</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line">object Spark06_SparkSQL_Test1 &#123;</span><br><span class="line"></span><br><span class="line">    def main(args: <span class="keyword">Array</span>[String]): Unit <span class="operator">=</span> &#123;</span><br><span class="line">        System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)</span><br><span class="line"></span><br><span class="line">        val sparkConf <span class="operator">=</span> <span class="keyword">new</span> SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;sparkSQL&quot;)</span><br><span class="line">        val spark <span class="operator">=</span> SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(&quot;use atguigu&quot;)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">              |select</span><br><span class="line">              |    *</span><br><span class="line">              |from (</span><br><span class="line">              |    select</span><br><span class="line">              |        *,</span><br><span class="line">              |        rank() over( partition by area order by clickCnt desc ) as rank</span><br><span class="line">              |    from (</span><br><span class="line">              |        select</span><br><span class="line">              |           area,</span><br><span class="line">              |           product_name,</span><br><span class="line">              |           count(*) as clickCnt</span><br><span class="line">              |        from (</span><br><span class="line">              |            select</span><br><span class="line">              |               a.*,</span><br><span class="line">              |               p.product_name,</span><br><span class="line">              |               c.area,</span><br><span class="line">              |               c.city_name</span><br><span class="line">              |            from user_visit_action a</span><br><span class="line">              |            join product_info p on a.click_product_id = p.product_id</span><br><span class="line">              |            join city_info c on a.city_id = c.city_id</span><br><span class="line">              |            where a.click_product_id &gt; -1</span><br><span class="line">              |        ) t1 group by area, product_name</span><br><span class="line">              |    ) t2</span><br><span class="line">              |) t3 where rank &lt;= 3</span><br><span class="line">            &quot;&quot;&quot;.stripMargin).<span class="keyword">show</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>第二步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.spark.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark06_SparkSQL_Test2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use atguigu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查询基本数据</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |  select</span></span><br><span class="line"><span class="string">              |     a.*,</span></span><br><span class="line"><span class="string">              |     p.product_name,</span></span><br><span class="line"><span class="string">              |     c.area,</span></span><br><span class="line"><span class="string">              |     c.city_name</span></span><br><span class="line"><span class="string">              |  from user_visit_action a</span></span><br><span class="line"><span class="string">              |  join product_info p on a.click_product_id = p.product_id</span></span><br><span class="line"><span class="string">              |  join city_info c on a.city_id = c.city_id</span></span><br><span class="line"><span class="string">              |  where a.click_product_id &gt; -1</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin).createOrReplaceTempView(<span class="string">&quot;t1&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据区域，商品进行数据聚合</span></span><br><span class="line">        spark.udf.register(<span class="string">&quot;cityRemark&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="type">CityRemarkUDAF</span>()))</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |  select</span></span><br><span class="line"><span class="string">              |     area,</span></span><br><span class="line"><span class="string">              |     product_name,</span></span><br><span class="line"><span class="string">              |     count(*) as clickCnt,</span></span><br><span class="line"><span class="string">              |     cityRemark(city_name) as city_remark</span></span><br><span class="line"><span class="string">              |  from t1 group by area, product_name</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin).createOrReplaceTempView(<span class="string">&quot;t2&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 区域内对点击数量进行排行</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |  select</span></span><br><span class="line"><span class="string">              |      *,</span></span><br><span class="line"><span class="string">              |      rank() over( partition by area order by clickCnt desc ) as rank</span></span><br><span class="line"><span class="string">              |  from t2</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin).createOrReplaceTempView(<span class="string">&quot;t3&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取前3名</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              | select</span></span><br><span class="line"><span class="string">              |     *</span></span><br><span class="line"><span class="string">              | from t3 where rank &lt;= 3</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.stripMargin).show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buffer</span>(<span class="params"> var total : <span class="type">Long</span>, var cityMap:mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] </span>)</span></span><br><span class="line">    <span class="comment">// 自定义聚合函数：实现城市备注功能</span></span><br><span class="line">    <span class="comment">// 1. 继承Aggregator, 定义泛型</span></span><br><span class="line">    <span class="comment">//    IN ： 城市名称</span></span><br><span class="line">    <span class="comment">//    BUF : Buffer =&gt;【总点击数量，Map[（city, cnt）, (city, cnt)]】</span></span><br><span class="line">    <span class="comment">//    OUT : 备注信息</span></span><br><span class="line">    <span class="comment">// 2. 重写方法（6）</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">String</span>, <span class="type">Buffer</span>, <span class="type">String</span>]</span>&#123;</span><br><span class="line">        <span class="comment">// 缓冲区初始化</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buffer</span> = &#123;</span><br><span class="line">            <span class="type">Buffer</span>(<span class="number">0</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新缓冲区数据</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buffer</span>, city: <span class="type">String</span>): <span class="type">Buffer</span> = &#123;</span><br><span class="line">            buff.total += <span class="number">1</span></span><br><span class="line">            <span class="keyword">val</span> newCount = buff.cityMap.getOrElse(city, <span class="number">0</span>L) + <span class="number">1</span></span><br><span class="line">            buff.cityMap.update(city, newCount)</span><br><span class="line">            buff</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并缓冲区数据</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buffer</span>, buff2: <span class="type">Buffer</span>): <span class="type">Buffer</span> = &#123;</span><br><span class="line">            buff1.total += buff2.total</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> map1 = buff1.cityMap</span><br><span class="line">            <span class="keyword">val</span> map2 = buff2.cityMap</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 两个Map的合并操作</span></span><br><span class="line"><span class="comment">//            buff1.cityMap = map1.foldLeft(map2) &#123;</span></span><br><span class="line"><span class="comment">//                case ( map, (city, cnt) ) =&gt; &#123;</span></span><br><span class="line"><span class="comment">//                    val newCount = map.getOrElse(city, 0L) + cnt</span></span><br><span class="line"><span class="comment">//                    map.update(city, newCount)</span></span><br><span class="line"><span class="comment">//                    map</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line">            map2.foreach&#123;</span><br><span class="line">                <span class="keyword">case</span> (city , cnt) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> newCount = map1.getOrElse(city, <span class="number">0</span>L) + cnt</span><br><span class="line">                    map1.update(city, newCount)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            buff1.cityMap = map1</span><br><span class="line">            buff1</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将统计的结果生成字符串信息</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buffer</span>): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> remarkList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> totalcnt = buff.total</span><br><span class="line">            <span class="keyword">val</span> cityMap = buff.cityMap</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 降序排列</span></span><br><span class="line">            <span class="keyword">val</span> cityCntList = cityMap.toList.sortWith(</span><br><span class="line">                (left, right) =&gt; &#123;</span><br><span class="line">                    left._2 &gt; right._2</span><br><span class="line">                &#125;</span><br><span class="line">            ).take(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> hasMore = cityMap.size &gt; <span class="number">2</span></span><br><span class="line">            <span class="keyword">var</span> rsum = <span class="number">0</span>L</span><br><span class="line">            cityCntList.foreach&#123;</span><br><span class="line">                <span class="keyword">case</span> ( city, cnt ) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> r = cnt * <span class="number">100</span> / totalcnt</span><br><span class="line">                    remarkList.append(<span class="string">s&quot;<span class="subst">$&#123;city&#125;</span> <span class="subst">$&#123;r&#125;</span>%&quot;</span>)</span><br><span class="line">                    rsum += r</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> ( hasMore ) &#123;</span><br><span class="line">                remarkList.append(<span class="string">s&quot;其他 <span class="subst">$&#123;100 - rsum&#125;</span>%&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            remarkList.mkString(<span class="string">&quot;, &quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buffer</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://lvxiaoyi.top">lvxiaoyi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lvxiaoyi.top/932a1be1.html">https://lvxiaoyi.top/932a1be1.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lvxiaoyi.top" target="_blank">吕小医's BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="https://img.lvxiaoyi.top/typora-img/202111082049932.png/lvxiaoyi" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/fa6bf3a7.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082146936.png/lvxiaoyi" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Scala基础-2</div></div></a></div><div class="next-post pull-right"><a href="/beeb1f38.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-9964d2894516902605191817111ec781_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Kafka基础</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/5eb2dc8b.html" title="hadoop基础-3Yarn"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/image-20211028174657424.png/lvxiaoyi" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">hadoop基础-3Yarn</div></div></a></div><div><a href="/1d83a7d1.html" title="Hive基础-1"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/image-20211028174657424.png/lvxiaoyi" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">Hive基础-1</div></div></a></div><div><a href="/bc13a2ae.html" title="HBase基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/image-20211028174657424.png/lvxiaoyi" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">HBase基础</div></div></a></div><div><a href="/beeb1f38.html" title="Kafka基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-9964d2894516902605191817111ec781_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">Kafka基础</div></div></a></div><div><a href="/9cbb2d93.html" title="hive基础-2"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/image-20211028174657424.png/lvxiaoyi" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">hive基础-2</div></div></a></div><div><a href="/6362a21d.html" title="Scala基础-1"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111082146936.png/lvxiaoyi" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">Scala基础-1</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p0.meituan.net/csc/8a1b1d488e6a8ab5108c9c78f36b3a1776955.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">lvxiaoyi</div><div class="author-info__description">ISFP到ESFJ</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">205</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lvxiaoyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">冲鸭！内卷起来了兄弟</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E5%9F%BA%E7%A1%80-2SparkSQL"><span class="toc-number">1.</span> <span class="toc-text">Spark基础-2SparkSQL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL-%E6%A6%82%E8%BF%B0"><span class="toc-number">2.</span> <span class="toc-text">SparkSQL 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkSQL-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">2.1.</span> <span class="toc-text">SparkSQL 是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-and-SparkSQL"><span class="toc-number">2.2.</span> <span class="toc-text">Hive and SparkSQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkSQL-%E7%89%B9%E7%82%B9"><span class="toc-number">2.3.</span> <span class="toc-text">SparkSQL 特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">2.4.</span> <span class="toc-text">DataFrame 是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">2.5.</span> <span class="toc-text">DataSet 是什么</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">SparkSQL 核心编程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E7%9A%84%E8%B5%B7%E7%82%B9"><span class="toc-number">3.1.</span> <span class="toc-text">新的起点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame"><span class="toc-number">3.2.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-DataFrame"><span class="toc-number">3.2.1.</span> <span class="toc-text">创建 DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SQL-%E8%AF%AD%E6%B3%95"><span class="toc-number">3.2.2.</span> <span class="toc-text">SQL 语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DSL-%E8%AF%AD%E6%B3%95"><span class="toc-number">3.2.3.</span> <span class="toc-text">DSL 语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataFrame"><span class="toc-number">3.2.4.</span> <span class="toc-text">RDD 转换为 DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame-%E8%BD%AC%E6%8D%A2%E4%B8%BA-RDD"><span class="toc-number">3.2.5.</span> <span class="toc-text">DataFrame 转换为 RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet"><span class="toc-number">3.3.</span> <span class="toc-text">DataSet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-DataSet"><span class="toc-number">3.3.1.</span> <span class="toc-text">创建 DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataSet"><span class="toc-number">3.3.2.</span> <span class="toc-text">RDD 转换为 DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet-%E8%BD%AC%E6%8D%A2%E4%B8%BA-RDD"><span class="toc-number">3.3.3.</span> <span class="toc-text">DataSet 转换为 RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame-%E5%92%8C-DataSet-%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.4.</span> <span class="toc-text">DataFrame 和 DataSet 转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E3%80%81DataFrame%E3%80%81DataSet-%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">3.5.</span> <span class="toc-text">RDD、DataFrame、DataSet 三者的关系</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7"><span class="toc-number">3.5.1.</span> <span class="toc-text">三者的共性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">3.5.2.</span> <span class="toc-text">三者的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.5.3.</span> <span class="toc-text">三者的互相转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IDEA-%E5%BC%80%E5%8F%91-SparkSQL"><span class="toc-number">3.6.</span> <span class="toc-text">IDEA 开发 SparkSQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96"><span class="toc-number">3.6.1.</span> <span class="toc-text">添加依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.6.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">3.7.</span> <span class="toc-text">用户自定义函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#UDF"><span class="toc-number">3.7.1.</span> <span class="toc-text">UDF</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UDAF"><span class="toc-number">3.8.</span> <span class="toc-text">UDAF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98"><span class="toc-number">3.9.</span> <span class="toc-text">数据的加载和保存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%96%B9%E5%BC%8F"><span class="toc-number">3.9.1.</span> <span class="toc-text">通用的加载和保存方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parquet"><span class="toc-number">3.9.2.</span> <span class="toc-text">Parquet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON"><span class="toc-number">3.9.3.</span> <span class="toc-text">JSON</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CSV"><span class="toc-number">3.9.4.</span> <span class="toc-text">CSV</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL"><span class="toc-number">3.9.5.</span> <span class="toc-text">MySQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive"><span class="toc-number">3.9.6.</span> <span class="toc-text">Hive</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL-%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98"><span class="toc-number">4.</span> <span class="toc-text">SparkSQL 项目实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">4.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%EF%BC%9A%E5%90%84%E5%8C%BA%E5%9F%9F%E7%83%AD%E9%97%A8%E5%95%86%E5%93%81-Top3"><span class="toc-number">4.2.</span> <span class="toc-text">需求：各区域热门商品 Top3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E7%AE%80%E4%BB%8B"><span class="toc-number">4.2.1.</span> <span class="toc-text">需求简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">4.2.2.</span> <span class="toc-text">需求分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.2.3.</span> <span class="toc-text">功能实现</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/560e8e48.html" title="博客美化"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-9964d2894516902605191817111ec781_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="博客美化"/></a><div class="content"><a class="title" href="/560e8e48.html" title="博客美化">博客美化</a><time datetime="2023-12-29T14:58:12.257Z" title="发表于 2023-12-29 22:58:12">2023-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/7ef7bbd4.html" title="LeetCode 118. 杨辉三角"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/202111011355394.png/lvxiaoyi" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode 118. 杨辉三角"/></a><div class="content"><a class="title" href="/7ef7bbd4.html" title="LeetCode 118. 杨辉三角">LeetCode 118. 杨辉三角</a><time datetime="2023-03-19T14:25:59.394Z" title="发表于 2023-03-19 22:25:59">2023-03-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/82c8cab7.html" title="shell基础"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-9964d2894516902605191817111ec781_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="shell基础"/></a><div class="content"><a class="title" href="/82c8cab7.html" title="shell基础">shell基础</a><time datetime="2022-10-12T16:07:18.841Z" title="发表于 2022-10-13 00:07:18">2022-10-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/f1601c3e.html" title="单例模式"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/1002892-20180912131026735-781767905.png/lvxiaoyi" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="单例模式"/></a><div class="content"><a class="title" href="/f1601c3e.html" title="单例模式">单例模式</a><time datetime="2022-09-12T07:22:48.777Z" title="发表于 2022-09-12 15:22:48">2022-09-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/b15f0f1b.html" title="设计模式基础-1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.lvxiaoyi.top/typora-img/image-20211012093705433.png/lvxiaoyi" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="设计模式基础-1"/></a><div class="content"><a class="title" href="/b15f0f1b.html" title="设计模式基础-1">设计模式基础-1</a><time datetime="2022-09-12T07:22:48.777Z" title="发表于 2022-09-12 15:22:48">2022-09-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://img.lvxiaoyi.top/typora-img/202111082049932.png/lvxiaoyi')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By lvxiaoyi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/lvxiaoyi.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>